{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm for CMV Delta Winners\n",
    "\n",
    "We will create a basic decision tree with the first decision rules to test:\n",
    "* Words of Certainty\n",
    "* Words of Extremity\n",
    "* Lexical Diversity\n",
    "* Total Number of Characters\n",
    "* Number of Provided Links\n",
    "* Whether or not OP was quoted\n",
    "\n",
    "#### TODO\n",
    "~~Decide how to calculate decision rules<br>~~\n",
    "~~Figure out what spans to use for:<br>~~\n",
    " * ~~Lexical Diversity~~\n",
    " * ~~Number of Chars~~\n",
    " * ~~Number of links~~\n",
    "\n",
    "Write func to clean body<br>\n",
    "Fix getNumQuotes()<br>\n",
    "Optimize Functions by limiting # of loops used on same file<br>\n",
    "Write values for decision tree into CSV file<br>\n",
    "Learn to read and write from csv file using pandas<br>\n",
    "\n",
    " \n",
    "#### Notes\n",
    "Given CSV files are encoded as UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize the loops so you only need to run through the text once\n",
    "\n",
    "# these are the words of certainty\n",
    "wOC = [\"absolutely\", \"always\", \"certain\", \"commit\", \"completely\", \"every\", \"exact\"] \n",
    "\n",
    "# this is a function that counts the number of words of certainty\n",
    "getCertaintyCount = lambda text,word_i=0: text.count(wOC[word_i]) if word_i == len(wOC) - 1 else text.count(wOC[word_i]) + getCertaintyCount(text, word_i+1)\n",
    "\n",
    "\n",
    "# initial words of extremity\n",
    "eWC = [\"much\", \"more\", \"extremely\", \"very\", \"wonderful\"]\n",
    "\n",
    "# this function counts the total of \n",
    "getExtremityCount = lambda text,word_i=0: text.count(eWC[word_i]) if word_i == len(eWC) - 1 else text.count(eWC[word_i]) + getExtremityCount(text, word_i+1)\n",
    "\n",
    "\n",
    "\n",
    "# here is the calculator for lexical diversity\n",
    "def getLexicalDiversity(text):\n",
    "    lstOfWords = text.split()\n",
    "    setOfWords = set(lstOfWords)\n",
    "    return len(setOfWords)/len(lstOfWords)\n",
    "\n",
    "\n",
    "# here is the counter for number of links\n",
    "def getNumLinks(text):\n",
    "    return text.count(\"<a\")\n",
    "\n",
    "\n",
    "# here is the counter for num of quotes\n",
    "def getNumQuotes(text):\n",
    "    return text.count(\"\\n> \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will test the functions created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words of certainty: 3\n",
      "Number of words of extremity: 4\n",
      "Lexical Diversity: 0.9545454545454546\n",
      "Number of links: 0\n",
      "Number of quotes: 0\n"
     ]
    }
   ],
   "source": [
    "text = \"i absolutely very very much love using words like certain commit and wonderful here is a link http://google.com. \\n>This person said this\"\n",
    "print(f\"Number of words of certainty: {getCertaintyCount(text)}\")\n",
    "print(f\"Number of words of extremity: {getExtremityCount(text)}\")\n",
    "print(f\"Lexical Diversity: {getLexicalDiversity(text)}\")\n",
    "print(f\"Number of links: {getNumLinks(text)}\")\n",
    "print(f\"Number of quotes: {getNumQuotes(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening and Testing the CSV FIle uing CSV Module\n",
    "\n",
    "We start here by attempting to open the file and read a certain amount of lines(3 lines).\n",
    "This is the basic setup as to how we're going to read through the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are subreddit, created_utc, author, id, parent_id, body, body_html, Delta_Awarded, Delta_Awarder, Delta_AwardText, Submission_Author\n",
      "\n",
      "\tAuthor:\n",
      "Thompson_S_Sweetback\n",
      "\tInitial Content:\n",
      "1. Long term economic disincentives will not be very effective because people are naturally optimist.\n",
      "\n",
      "1358441654\n",
      "\tAuthor:\n",
      "llatia\n",
      "\tInitial Content:\n",
      "I think most people would say that whether they act on desires and thoughts or not is precisely the .\n",
      "\n",
      "1358460613\n",
      "Processed 3 lines.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "lines_to_read = 3\n",
    "with open('TextData.csv', mode='r', encoding=\"utf-8\") as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}\\n')\n",
    "            line_count += 1\n",
    "        print(f'\\tAuthor:\\n{row[\"author\"]}\\n\\tInitial Content:\\n{row[\"body\"][0:100]}.\\n')\n",
    "        line_count += 1\n",
    "        if line_count >= lines_to_read:\n",
    "              break;\n",
    "    print(f'Processed {line_count} lines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the span for Lexical Diversity and Number of Characters\n",
    "\n",
    "Manipulating data gets boring if that's all you do, so I'll start by creating a representation of both from the first 100 comments. \n",
    "\n",
    "<font color=blue>***Let's have some fun!***</font>\n",
    "\n",
    "* Will multiply Lexical Diversity by 100, then round to the nearest 10\n",
    "* Will round number of characters to the nearest 1000\n",
    "\n",
    "The visual representation will be the rounded numbers by the amount of occurences of those numbers\n",
    "\n",
    "We will start by filling in the dictionaries lexicalDiversity and numOfChars with the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are subreddit, created_utc, author, id, parent_id, body, body_html, Delta_Awarded, Delta_Awarder, Delta_AwardText, Submission_Author\n",
      "\n",
      "Rounded number of characters: Occurences\n",
      "0: 506 | 1000: 334 | 2000: 87 | 3000: 41 | 4000: 18 | 5000: 3 | 6000: 4 | 7000: 2 | 8000: 2 | 9000: 1 | 10000: 1  \n",
      "\n",
      "Rounded lexical diversities: Occurences\n",
      "40.0: 6 | 50.0: 41 | 60.0: 168 | 70.0: 227 | 80.0: 220 | 90.0: 186 | 100.0: 151  \n",
      "\n",
      "Processed 1000 lines.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "lines_to_read = 1000\n",
    "\n",
    "lexicalDiversity = {}\n",
    "numOfChars = {}\n",
    "\n",
    "with open('TextData.csv', mode='r', encoding=\"utf-8\") as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}\\n')\n",
    "            line_count += 1\n",
    "                  \n",
    "        # file manipulation here\n",
    "        #use row[column_name] to get data within that column\n",
    "                  \n",
    "        #this shows the first 100 chars of data in the comments\n",
    "        # print(f'\\tAuthor:\\n{row[\"author\"]}\\n\\tInitial Content:\\n{row[\"body\"][0:100]}.\\n') \n",
    "                  \n",
    "        text_length = len(row[\"body\"])\n",
    "        text_length = round(text_length, -3)\n",
    "        if text_length not in numOfChars.keys():\n",
    "              numOfChars[text_length] = 1\n",
    "        else:\n",
    "              numOfChars[text_length] += 1\n",
    "        \n",
    "        cur_lex_div = getLexicalDiversity(row[\"body\"])\n",
    "        cur_lex_div = round(100 * cur_lex_div, -1)\n",
    "        if cur_lex_div not in lexicalDiversity.keys():\n",
    "              lexicalDiversity[cur_lex_div] = 1\n",
    "        else:\n",
    "              lexicalDiversity[cur_lex_div] += 1\n",
    "              \n",
    "        #file manipulation ends here\n",
    "        line_count += 1\n",
    "        if line_count >= lines_to_read:\n",
    "              break;\n",
    "        #print(line_count) \n",
    "    print(\"Rounded number of characters: Occurences\")\n",
    "    \n",
    "    result = \"\"\n",
    "    for x,y in sorted(numOfChars.items()):\n",
    "        result += f\"{x}: {y} | \"\n",
    "    print(result[:-2],\"\\n\")\n",
    "    #print(numOfChars,\"\\n\")\n",
    "    \n",
    "    print(\"Rounded lexical diversities: Occurences\")\n",
    "    result = \"\"\n",
    "    for x,y in sorted(lexicalDiversity.items()):\n",
    "        result += f\"{x}: {y} | \"\n",
    "    print(result[:-2],\"\\n\")\n",
    "        \n",
    "    print(f'Processed {line_count} lines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to write some data to a csv file...\n",
    "\n",
    "We're going to first write our headers to our first row, which will be:\n",
    "(subreddit, author, parent_id, id, Delta_Awarded, body, body_html, certainty_count, extremity_count, \n",
    "    lexical_diversity_rounded, char_count_rounded, link_count, quote_count)\n",
    "\n",
    "Then we're gonna go through and fill in our data!\n",
    "I'll write to a file called \"OutputTest.csv\" for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header row written.\n",
      "Initial columns being read: subreddit, created_utc, author, id, parent_id, body, body_html, Delta_Awarded, Delta_Awarder, Delta_AwardText, Submission_Author\n",
      "\n",
      "Processed 3 lines.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "write_file_name = \"OutputTest.csv\"\n",
    "file = open(write_file_name, mode=\"w\")\n",
    "writer = csv.writer(file, delimiter=',', quotechar='\"')\n",
    "\n",
    "#first writer header row\n",
    "writer.writerow(['subreddit', 'author', 'parent_id', 'id', 'Delta_Awarded', 'body', 'body_html', 'certainty_count', 'extremity_count', \n",
    "    'lexical_diversity_rounded', 'char_count_rounded', 'link_count', 'quote_count'])\n",
    "\n",
    "print(\"Header row written.\")\n",
    "\n",
    "lines_to_read = 3\n",
    "with open('TextData.csv', mode='r', encoding=\"utf-8\") as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Initial columns being read: {\", \".join(row)}\\n')\n",
    "            line_count += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculate certainty_count, extremity_count, lexical_diversity_rounded, char_count_rounded, link_count, quote_count\n",
    "                  \n",
    "        certainty_count = getCertaintyCount(row['body'])\n",
    "        extremity_count = getExtremityCount(row['body'])\n",
    "        lexical_diversity = getLexicalDiversity(row[\"body\"])\n",
    "        lexical_diversity_rounded = round(100 * lexical_diversity, -1)\n",
    "        char_count_rounded = round(len(row[\"body\"]), -3)\n",
    "        link_count = getNumLinks(row[\"body_html\"])\n",
    "        quote_count = getNumQuotes(row[\"body\"])\n",
    "                  \n",
    "        #column order: (subreddit, author, parent_id, id, Delta_Awarded, body, body_html, certainty_count, extremity_count, \n",
    "        #lexical_diversity_rounded, char_count_rounded, link_count, quote_count)\n",
    "        writer.writerow([row['subreddit'], row['author'], row['parent_id'], row['id'], row['Delta_Awarded'], row['body'], row['body_html'],\n",
    "                                  certainty_count, extremity_count, lexical_diversity_rounded, char_count_rounded, link_count, quote_count])\n",
    "                  \n",
    "        \n",
    "        line_count += 1\n",
    "        if line_count >= lines_to_read:\n",
    "              break;\n",
    "    print(f'Processed {line_count} lines.')\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
